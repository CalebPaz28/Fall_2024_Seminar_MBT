---
title: RRV Project
output: html_document
---

```{r include = FALSE}
library(rmarkdown)
library(tidyverse)
library(patchwork)
```

# **Purpose**

There are many research projects conducted on vastly different organisms that are not related to virus research. Because the focus on a given research task does not involve the detection or discovery of viruses, this is often overlooked and provides an opportunity for virologists to comb through the data and see if viruses are present. Given that each organism present on plant Earth is estimated to be infected by at least one virus, this provides ample opportunity to discover novel viruses in a host of interest.

Not only is this conceptual, but this as been applied in a variety of hosts. I will focus on viruses isolated from roses in this case.

In roses,

Xing, F., Gao, D., Habili, N., Wang, H., Zhang, Z., Cao, M., & Li, S. (<mark>2021</mark>). Identification and molecular characterization of a novel carlavirus infecting rose plants (Rosa chinensis Jacq.). Archives of Virology, 166, 3499-3502.

- RVC was found by looking through transcriptome data
- This can help in the future as growers and diagnosticians decide which viruses they should screen for.

# **Exploring RNA-seq data for the presence of unreported or novel viruses in the genus *Rosa* **

## **Overview of the project**

1. Download SRA data (RNA-seq) -> using the array system
2. Download Reference genome
3. QC and trimming of raw data
4. Reference mapping of reads to host genome(Hisat2 which is best used for RNA onto DNA: think about exons)
5. Gather unmapped reads using the flags (may have to convert the SAM to a fastq file for assembly)
6. Assembly of reads (rnaSPADES)
7. Create a database of viruses (using the taxa accession or ID) so that BLAST can be run locally
8. Use Diamond for blasting (requires protein sequences: translate the nucleic acid to protein)
9. Accumulate BLAST results for both nucleotide and protein blast
10. Analyze results


> I am planning to maintain everything in the scratch folder on the OSCER. Also, I am keeping all the scripts in my Rstudio folder so that I can easily upload the correct scripts while correcting them when submitting jobs.


### **1. Download SRA Data (RNA-Seq)**


1. Login to OSCER

```bach
ssh [YOUR_ACCOUNT_ID]@schooner.oscer.ou.edu
```

2. Activate environment

```bach
mamba activate /home/mbtoomey/.conda/envs/BIOL7263_Genomics
```

3. Submit a job to OSCER

```bach
sbatch /scratch/biol726310/BIOL7263_Genomics/rrv_project/scripts/[file_name.sbatch]
```

4. Check on a job

```bach
squeue -u biol726310
```

5. We can include this line of code in our script.

```bach
module load SRA-Toolkit/3.0.3-gompi-2022a
```

6. The following code is our `.sbatch` file and contains the `array` argument. This must be altered according to the number of individual jobs being submitted.  

Below are the files submitted to OSCER:

1) **[sbatch file](scripts/fastq_dump.sbatch)** 

2) **[sh script](scripts/fastq_dump.sh)**

3) **[.args](scripts/fastq_dump.args)**

7. Here is our `.sh` script file which contains a line of code that specifies a loading of the SRA-Toolkit. This program will be used to obtain data associated with a specific accessions.

8. Lastly, we have a `.args` file which contains a list of information that will be run using the array system.

In this case, we set the array argument to `1-3`. This means, the array will only process the first three arguments in the `.args` file.

*___________________________________________________________________________________________________*

### **2. Downloading the Reference genome of interest**

In this case, we will use the **Rosa** genome as a reference. This reference corresponds with the RNA-seq data downloaded in the previous section.

We can manually download the data of interest, or download using the `wget` command and the respective link. Below is from our class exercises.

Steps:
1. Go to NCBI 
2. Go the genomes 
3. Search organism of interest 
4. Select FTP 
5. Copy link for file of interest (in our case, we will copy the link for the genomic.fna.gz, we may also want to grab the genomic.gff.gz) 


1) **[sh script](scripts/rose_reference.sh)**

2) **[sbatch file](scripts/rose_reference.sbatch)** 

Once these are downloaded, we will likely need to unzip them for further analysis. In this case, we will use an `gunzip` command, which we have incorporated into the above script.

### **3. QC and trimming of raw data**

As part of the analysis, it is important to consider the quality of the reads being analyzed. This is publicly available data, therefore, I assume the data is of high quality (otherwise, why publish?). Either way, we will still work through the process of data quality control and trimming of the raw fastq files (for sake of practice and understanding).

To view the first few headers we can use the `zcat` command (this is similar to 'cat' but works with zipped files). We can also use `head` and `tail` to look at the data. So in the case that our file is NOT zipped, we will use `cat`, otherwise we will use `zcat`.

```bach
cat /scratch/biol726310/BIOL7263_Genomics/rrv_project/SRR29872023_1.fastq | head | grep @SRR
cat /scratch/biol726310/BIOL7263_Genomics/rrv_project/SRR29872023_2.fastq | head | grep @SRR
```

Check that there is an identical number of reads in each file (this one is not working):

```bach
cat /scratch/biol726310/BIOL7263_Genomics/rrv_project/SRR29872023_2.fastq | grep @SRR | wc â€“l
```

From our class exercises: We will need to create `.sh` and `.sbatch` files with our code. Generally anything with a median quality score greater than Q20 is regarded as acceptable; anything above Q30 is regarded as 'good'.

Below are the files submitted to OSCER:

1) **[sbatch file](scripts/rose_fastqc.sbatch)** 

2) **[sh script](scripts/rose_fastqc.sh)**

Here we are only looking at one sample, which is `SRR29872023_1` and `SRR29872023_2`. If we wanted to use an array system, we could look at all of the files and have `fastqc` run for each of them.

> If I complete the main project, I will use an array at this step (likely other steps downstream) to process a larger number of samples.

#### **Results of the quality control using fastqc**

<iframe src="data_output/SRR29872023_1_fastqc.html" width="100%" height="600px"></iframe>

<iframe src="data_output/SRR29872023_2_fastqc.html" width="100%" height="600px"></iframe>

### **Trimming**

We will create a new directory where we can store the trimmed data and simply call it `trimmed_data`.

```bach
mkdir /scratch/biol726310/BIOL7263_Genomics/rrv_project/trimmed_data
```

We will now create the script that we can use to initate the trimming program.

Below are the files submitted to OSCER:

1) **[sh script](scripts/rose_trimming.sh)** 

2) **[sbatch file](scripts/rose_trimming.sbatch)**


<iframe src="data_output/trimmed_reads_val_1_fastqc.html" width="100%" height="600px"></iframe>

<iframe src="data_output/trimmed_reads_val_2_fastqc.html" width="100%" height="600px"></iframe>


*___________________________________________________________________________________________________*


### **4. Reference mapping of reads to host genome**

Usually, we would use BWA (currently called `minimap2`) to map reads to a reference genome. We will be using Hisat2 for read mapping because this program is particular good at mapping RNA reads onto a DNA genome.

General steps:
1) Create a reference index of the reference sequence (host sequence in most of our cases)

2) Map raw, trimmed, de novo assembled (any reads) to the indexed reference sequence

3) The output will be a `.SAM` file (Simple Alignment format)

4) Convert the `.SAM` to a `.BAM` (Binary alignment format). `.SAM` files are great for readability but they do not do so well with computational accessibility. This is why we convert the file to `.BAM`. 

5) Sort the `.BAM` file.

The below code is from Salil:

1. creating a new environment
```bach
conda create -n hisat2_env
```

2. activate the environment
```bach
conda activate hisat2_env
```

> `mamba` can also be used instead of `conda`

3. We will likely have to configure the channels that will be required for package installation.

```bach
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --add channels defaults
```

4. We can check the channels with the following commands. The order is important

```bach
conda config --show channels
```

5. Install a program of interest (hisat2)
```bach
conda install hisat2
```

6. unzip the file of interest
```bach
gunzip [file of interest, usually *.gz]
```

7. create an index file
```bach
hisat2-build /scratch/biol726310/BIOL7263_Genomics/rrv_project/reference_genome/GCA_002994745.2_RchiOBHm-V2_genomic.fna /scratch/biol726310/BIOL7263_Genomics/rrv_project/reference_genome/indexed_genome
```

8. Aligning reads to the indexed reference genome

```bach
hisat2 -x /scratch/biol726310/BIOL7263_Genomics/rrv_project/reference_genome/indexed_genome -1 /scratch/biol726310/BIOL7263_Genomics/rrv_project/trimmed_data/trimmed_reads_val_1.fq.gz -2 /scratch/biol726310/BIOL7263_Genomics/rrv_project/trimmed_data/trimmed_reads_val_2.fq.gz -S /scratch/biol726310/BIOL7263_Genomics/rrv_project/hisat2.sam
```

Generic form of the command.

```bach
hisat2 -x indexed_genome -1 reads_1.fastq -2 reads_2.fastq -S output.sam
```
`-x index_basename`: The name of the genome index files (from the indexing step).
`-1 reads_1.fastq`: The file containing the first reads of the paired-end data (these might have to be decompressed or `gunzipped`).
`-2 reads_2.fastq`: The file containing the second reads of the paired-end data (these might have to be decompressed or `gunzipped`).
`-S output.sam`: The name of the output file in SAM format.
`--dta`: This option enables HISAT2 to output alignments tailored for transcript assembly with tools like StringTie.
`--threads`: Specifies the number of threads to use for faster alignment:

Below I have included the script file for the `hisat2` mapping. 

1) **[sh script](scripts/hisat_mapping.sh)** 

2) **[sbatch file](scripts/hisat_mapping.sbatch)**

We will need to deactivate the current environment and activate the class environment to finish the remaining steps.

Activate environment

```bach
mamba deactivate
```

```bach
mamba activate /home/mbtoomey/.conda/envs/BIOL7263_Genomics
```

9. Convert the `.SAM` to `.BAM`

```bach
samtools view -S -b /scratch/biol726310/BIOL7263_Genomics/rrv_project/hisat2.sam > /scratch/biol726310/BIOL7263_Genomics/rrv_project/hisat2.bam
```

`-S`: Indicates that the input is in SAM format.
`-b`: Specifies that the output should be in BAM format.

10. Sorting the `.BAM` file.

```bach
samtools sort /scratch/biol726310/BIOL7263_Genomics/rrv_project/hisat2.bam -o /scratch/biol726310/BIOL7263_Genomics/rrv_project/sorted_hisat2.bam
```

11. Indexing the `.BAM` file

Indexing the sorted BAM file allows fast access for viewing alignments or further processing (e.g., in tools like IGV or featureCounts).

```bach
samtools index /scratch/biol726310/BIOL7263_Genomics/rrv_project/sorted_hisat2.bam
```

12. We can check the alignment statistics using a few tools.

```bach
samtools flagstat /scratch/biol726310/BIOL7263_Genomics/rrv_project/sorted_hisat2.bam > /scratch/biol726310/BIOL7263_Genomics/rrv_project/mappingstats.txt
```

> This worked well (very fast also)!

```{r fig.cap="Looking at the mapping stats generated by smatools"}
# Specify the path to your text file
file_path <- "data_output/mappingstats.txt"

# Read the text file
text_content <- readLines(file_path)

# Print the content of the text file
cat(head(text_content, 30), sep = "\n")
```


We can also use the qualimap program to check the alignment.

```bach
qualimap bamqc -outdir /scratch/biol726310/BIOL7263_Genomics/rrv_project/bamqc -bam /scratch/biol726310/BIOL7263_Genomics/rrv_project/sorted_hisat2.bam -gff /scratch/biol726310/BIOL7263_Genomics/rrv_project/reference_genome/GCA_002994745.2_RchiOBHm-V2_genomic.gff
```

> Works well (slow processing)

```{r fig.cap="Looking at the mapping stats generated by smatools"}
# Specify the path to your text file
file_path <- "data_output/genome_results.txt"

# Read the text file
text_content <- readLines(file_path)

# Print the content of the text file
cat(head(text_content, 30), sep = "\n")
```

The following code has been added to a script file that contains all of the steps in one document. The comments need to be removed, but the code can be run multiple lines at a time (the only problem is the use of different environments, otherwise, we could run it all together).

1) **[sh script](scripts/sam_bam_sort_in.sh)** 

2) **[sbatch file](scripts/sam_bam_sort_in.sbatch)**

- Because we are using two different environments, I will have to change environments after this step. (If I created an environment with all of the programs in one, I could easily use the same environment for all the anlayses).

```bach
mamba deactivate
```

```
mamba activate hisat2_env
```
Generic formula for the data input.

```bach
hisat2 -x genome_index -1 sample_1.fastq -2 sample_2.fastq -S alignment.sam --summary-file alignment_summary.txt
```

```bach
hisat2 -x /scratch/biol726310/BIOL7263_Genomics/rrv_project/reference_genome/indexed_genome -1 /scratch/biol726310/BIOL7263_Genomics/rrv_project/trimmed_data/trimmed_reads_val_1.fq -2 /scratch/biol726310/BIOL7263_Genomics/rrv_project/trimmed_data/trimmed_reads_val_2.fq -S /scratch/biol726310/BIOL7263_Genomics/rrv_project/hisat2.sam --summary-file /scratch/biol726310/BIOL7263_Genomics/rrv_project/alignment_summary.txt
```
> Works well (very slow processing)!

```{r fig.cap="Looking at the mapping stats generated by smatools"}
# Specify the path to your text file
file_path <- "data_output/alignment_summary.txt"

# Read the text file
text_content <- readLines(file_path)

# Print the content of the text file
cat(head(text_content, 30), sep = "\n")
```

Below I have included the files for OSCER submission. The `hisat2` version for checking the alignment works very similar to the `qualimap` alignment check.


1) **[sh script](scripts/hisat_alignment_sum.sh)** 

2) **[sbatch file](scripts/hisat_alignment_sum.sbatch)**

*___________________________________________________________________________________________________*


### **5. Gather unmapped reads using the flags**

We will start by creating a new directory to store the unmapped reads.

```bach
mkdir unmapped
```

We need to first extract the unmapped reads from the file

```bach
samtools view -b -f 12 /scratch/biol726310/BIOL7263_Genomics/rrv_project/sorted_hisat2.bam -o /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped.bam
```

> Apparently, according to other sources, the `-f` argument should be `4` instead of `12` when considering an unpaired read. The argument `12` is used to obtain unmapped reads from both pairs.


We will need to then convert the unmapped reads back to a fastq format so that the SPAdes assembler can read the input file. We can convert `.BAM` to fastq using bedtools or samtools.

```bach
bedtools bamtofastq -i /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped.bam -fq /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/unmapped_r1.fastq -fq2 /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/unmapped_r2.fastq
```

> This worked very well.

```bach
# Convert the .BAM file to a format (using samtools) that can be read by SPAdes such as fastq
samtools fastq /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/unmapped.bam -1 /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/output_1.fastq -2 /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/output_2.fastq
```

> This worked very well.

I was able to use both tools (bedtools and samtools) to generate fastq files. Below are the documents submitted to OSCER:

1) **[sh script](scripts/unmapped_reads.sh)** 

2) **[sbatch file](scripts/unmapped_reads.sbatch)**


*___________________________________________________________________________________________________*


### **6. Assembly of unmapped reads (rnaSPADES)**

First we will make a directory to contain our assembly data.

```bach
mkdir assembly
```

```bach
spades.py --rna --threads 24 -o /scratch/biol726310/BIOL7263_Genomics/rrv_project/assembly/spades_assembly -1 /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/output_1.fastq -2 /scratch/biol726310/BIOL7263_Genomics/rrv_project/unmapped/output_2.fastq
```

Apparently, we can process multiple fastq files in one assembly step. This may be useful down to the road. Otherwise, we may use the array system.

```bach
spades.py --rna --threads 24 -o output_directory -1 reads_1_1.fastq.gz -2 reads_1_2.fastq.gz -1 reads_2_1.fastq.gz -2 reads_2_2.fastq.gz
```

Check how the assembly went using QUAST.

```bach
quast.py --output-dir /scratch/biol726310/BIOL7263_Genomics/rrv_project/assembly/spades_assembly/quast /scratch/biol726310/BIOL7263_Genomics/rrv_project/assembly/spades_assembly/transcripts.fasta
```

```{r fig.cap="Looking at the mapping stats generated by smatools"}
# Specify the path to your text file
file_path <- "data_output/quast_report.txt"

# Read the text file
text_content <- readLines(file_path)

# Print the content of the text file
cat(head(text_content, 30), sep = "\n")
```

1) **[sh script](scripts/assembly_quast.sh)**

2) **[sbatch file](scripts/assembly_quast.sbatch)**

*___________________________________________________________________________________________________*

### **7. Create a database of viruses (using the taxa accession or ID) so that BLAST can be run locally**

#### Nucleotide BLAST

To search a nucleotide database we can use the blast+ suite of tools. This is the exact same tool that is used for online blast searches. However, when we implement it on our system we will need to provide a search database. The blast+ suit includes a script to download the preconfigured databases from NCBI: [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569850/)

We will likely have to navigate to the directory where we plan to create the databases.

To check the databases that are available you can run:

```bach
update_blastdb.pl --showall
```

```bach
update_blastdb.pl --decompress [DATABASE NAME]
```

```bach
update_blastdb.pl --decompress nt_viruses
```

It may be required to unzip the previous file.

```
tar -xvzf nt_viruses.00.tar.gz
```

*core_nt* is the default database used in online blast searches, but is >160Gb. This is not feasible for our searches. We can download the *nt_viruses* or *nt_prok* databases, which are much smaller in relative size. The *nt_euk* is very much impractically large.

We can create a smaller database that contains certain viruses to reduce the search time. We also may be interested in fungi, bacteria, phytoplasma, etc.

Rather than relying on the above process (which were difficult to integrate), I simply downloaded nucleotide sequences manually for all viruses using NCBI.

```bach
makeblastdb -in human_RNA.fna -parse_seqids -blastdb_version 5 -title "Human RNA" -dbtype nucl -out human_rna_db
```

Now that we have generated the nucleotide database, we can use the `blastn` command to search the database for homologous sequences.

```bach
blastn -db /scratch/biol726310/BIOL7263_Genomics/rrv_project/databases/virus_db -query transcripts.fasta -outfmt "6 qseqid sseqid stitle" -num_threads 20 -num_alignments 1 > TTC_rna_blast.tsv
```

In the following code, I have written it in such a way as to generate a csv file that can be analyzed easily.

```bach
blastn -query /scratch/biol726310/BIOL7263_Genomics/rrv_project/assembly/spades_assembly/transcripts.fasta -db /scratch/biol726310/BIOL7263_Genomics/rrv_project/databases/virus_db/nt_viruses -out /scratch/biol726310/BIOL7263_Genomics/rrv_project/blasting/nt_results.csv -evalue 1e-5 -max_target_seqs 5 -num_threads 20 -outfmt "10 qseqid stitle pident length mismatch gapopen qstart qend sstart send evalue bitscore"
```

`qseqid`: Query sequence ID.
`sseqid`: Subject (database) sequence ID.
`pident`: Percentage of identical matches.
`length`: Alignment length of nucleotides OR amino acid residues
`mismatch`: Number of mismatches.
`gapopen`: Number of gap openings.
`qstart`: Start of alignment in the query.
`qend`: End of alignment in the query.
`sstart`: Start of alignment in the subject (database).
`send`: End of alignment in the subject.
`evalue`: Expect value (E-value).
`bitscore`: Bit score.

Below I have included my files for OSCER submission:

1) **[.sh script](scripts/nt_protein_blast.sh)** 

2) **[.sbatch file](scripts/nt_protein_blast.sbatch)**


*___________________________________________________________________________________________________*

### **8. Use Diamond for blasting against protein sequences (like BLASTx)**

We will use Diamond to run efficient blast searches of databases. Similar to the nucleotide database, we are going to build our own database for this search. To do this lets go to the Uniprotkb database and download all of the protein sequences for our organisms of interest.

We can download the sequences as `.fasta` files to construct our database. We can download from uniprotkb or NCBI.

Here is the taxon ID for viruses 10239. I was able to generate a link from the Uniprotkb website that will download all of the protein sequences using the link (rather than downloading manually).

Below I have included my files for OSCER submission:

1) **[.sh script](scripts/protein_db_download.sh)** 

2) **[.sbatch file](scripts/protein_db_download.sbatch)**

This is how we can join two databases (more than one organism of interest). We can join the data set, and then create the database using the below code.

```bach
cat ZeFi_proteins.fasta chicken_proteins.fasta > bird_proteins.fasta
```

Here is the command for building the database. This step is critical as it will generate the `.dmnd` formatted file.

```bach
diamond makedb --in /scratch/biol726310/BIOL7263_Genomics/rrv_project/databases/protein_db/all_virus_protein -d /scratch/biol726310/BIOL7263_Genomics/rrv_project/databases/protein_db/virus_protein_db
```

`-d` sets the path to our database.
`-k` limits the output to the top hit only.
`-q` sets the path to our query sequences from the genome
`-o` sets the name of the output file
`--threads` sets the number of cpu cores to use for the anlysis. Remember to match this in the .sbatch file.
`--outfmt 6` sets the output to a table.

Now we can run the blast search, however this time we will be searching RNA transcripts against a protein database. Therefore, we will use the blastx search option in diamond. The following code is supposed to generate a `.csv` file type that will be much easier to analyze and can be quickly summarized.

```bach
diamond blastx -d /scratch/biol726310/BIOL7263_Genomics/rrv_project/databases/protein_db/virus_protein_db.dmnd -q /scratch/biol726310/BIOL7263_Genomics/rrv_project/assembly/spades_assembly/transcripts.fasta --outfmt 6 qseqid stitle pident length mismatch gapopen qstart qend sstart send evalue bitscore --max-target-seqs 5 --threads 20 | sed 's/\t/,/g' > /scratch/biol726310/BIOL7263_Genomics/rrv_project/blasting/diamond_results.csv
```

Below I have included my files for OSCER submission:

> These files include both nucleotide and diamond blast searches on the same file. This makes it easier to simply run one job and get both results as output.

1) **[.sh script](scripts/nt_protein_blast.sh)** 

2) **[.sbatch file](scripts/nt_protein_blast.sbatch)**

*___________________________________________________________________________________________________*

### **9. Analyze BLAST and Diamond results for both nucleotide and amino acid sequences**

- I was able to generate a `.csv` output so that I can quickly evaluate the data generated using R. The goal is to easily render a document that summarizes our findings.

- Using R, we manipulated the input `.csv` file to make it more user friendly, and to include important sequencing results.

```{r}
protein_df <- read.csv("data_output/diamond_results.csv", header = FALSE)

colnames(protein_df) <- c("Query_ID","Seq_Title", "Percent_ID", "Align_Length", "Mismatch", "Gap_Open", "Query_Start", "Query_End", "Seq_Start","Send", "Evalue", "bitscore")


df_split <- protein_df %>% 
  mutate(NODE = str_extract(Query_ID,"NODE_[0-9]+"),
         LENGTH = 
str_extract(Query_ID, "length_[0-9]+"),
         COV = 
str_extract(Query_ID, "cov_[0-9]+\\.[0-9]+"))


df_cleaned <- df_split %>% 
  mutate(
    NODE = gsub("NODE_", "", NODE),      # Remove NODE_ prefix
    LENGTH = gsub("length_", "", LENGTH), # Remove length_ prefix
    COV = gsub("cov_", "", COV))           # Remove cov_ prefix


## Cleaning up the sequence title by extracting the OS portion
df_cleaned <- df_cleaned %>% 
  mutate(Seq_Title_Org = str_extract(Seq_Title, "OS=([^O]+)"))


df_complete <- df_cleaned %>%
  select("Seq_Title_Org","NODE","LENGTH","COV","Percent_ID","Align_Length","Mismatch","Gap_Open","Evalue")

## Changing the variable type of one variable
df_complete$NODE <- as.numeric(df_complete$NODE)
df_complete$LENGTH <- as.numeric(df_complete$LENGTH)
df_complete$COV <- as.numeric(df_complete$COV)
df_complete$Percent_ID <- as.numeric(df_complete$Percent_ID)
df_complete$Align_Length <- as.numeric(df_complete$Align_Length)
df_complete$Mismatch <- as.numeric(df_complete$Mismatch)
df_complete$Gap_Open <- as.numeric(df_complete$Gap_Open)

summary(df_complete)
```

> Now that we can look at the data obtained from the search and summarize it. We can look at a couple of parameters such as the coverage, percent ID, length, mismatch, etc.

```{r, eval=FALSE}
head(df_complete %>% arrange(desc(LENGTH)),10)
```

```{r}
head(df_complete %>% arrange(desc(COV)),10)
```

```{r}
head(df_complete %>% arrange(desc(Percent_ID)),10)
```

```{r}
head(df_complete %>% arrange(desc(Align_Length)),10)
```

```{r}
head(df_complete %>% arrange(Mismatch),10)
```

```{r patchwork-plot, fig.align="center"}

plot_1 <- df_complete %>%
  ggplot(aes(x = LENGTH))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("A")

plot_2 <- df_complete %>% filter(COV >= 5) %>% 
  ggplot(aes(x = COV))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("B")

plot_3 <- df_complete %>% ggplot(aes(x = Percent_ID))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("C")

plot_4 <- df_complete %>% ggplot(aes(x = Align_Length))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("D")

plot_5 <- df_complete %>% ggplot(aes(x = Mismatch))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("E")

(plot_1 | plot_2) / (plot_3 | plot_4 | plot_5) + plot_annotation("Search Results") # "|" arranges side-by-side, "/" arranges on top of each other
```

We can filter by a specific observations. This is a very useful tool, as we can obtain all of the nodes (contigs) that correspond to a given virus or other organism.

```{r}
df_complete %>% filter(Seq_Title_Org == "OS=Rose partitivirus ") ## Because of the manipulations 

df_complete %>% filter(Seq_Title_Org == "OS=Rose cryptic virus 1 ") ## Because of the manipulations 
```

We can reduce the data set so that we can easily manage the analysis of the different sequence hits. In this case, we reduced the data set to one sequence hit per node (using the percent identity as a variable that would dictate the selection of a specific node). Obviously, we could have this from the start when doing the initial diamond blast, but its nice to also see some of the other items that could possibly have homologous sequences.

```{r patchwork-plot2, fig.align="center"}
reduced_data <- df_complete %>%
    group_by(NODE) %>%
    slice_max(Percent_ID, n = 1, with_ties = TRUE) %>%
    ungroup()  # Remove the grouping structure

plot_1 <- reduced_data %>%
  ggplot(aes(x = LENGTH))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("A")

plot_2 <- reduced_data %>% filter(COV >= 5) %>% 
  ggplot(aes(x = COV))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("B")

plot_3 <- reduced_data %>% ggplot(aes(x = Percent_ID))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("C")

plot_4 <- reduced_data %>% ggplot(aes(x = Align_Length))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("D")

plot_5 <- reduced_data %>% ggplot(aes(x = Mismatch))+
  geom_histogram(fill = "skyblue", color = "black")+
  theme_classic()+
  ggtitle("E")

(plot_1 | plot_2) / (plot_3 | plot_4 | plot_5)  # "|" arranges side-by-side, "/" arranges on top of each other
```

```{r}
reduced_data <- df_complete %>%
    group_by(NODE) %>%
    slice_max(Percent_ID, n = 1, with_ties = TRUE) %>%
    ungroup()  # Remove the grouping structure

head(reduced_data %>% arrange(desc(COV)),10)

head(reduced_data %>% arrange(desc(Percent_ID)),10)

head(reduced_data %>% filter(COV >= 10) %>% 
  arrange(desc(Percent_ID)),10)
```

```{r}
new_df <- reduced_data %>% pivot_longer(cols = LENGTH:Mismatch, names_to = "Parameter", values_to = "Value")


new_df %>% ggplot(aes(x = Value, y = Parameter, fill = Parameter))+
  geom_boxplot()+
  theme_bw()
```



*___________________________________________________________________________________________________*









