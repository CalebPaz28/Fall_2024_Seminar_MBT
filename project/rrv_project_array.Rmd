---
title: "rrv_project_array"
author: "Paslay"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rmarkdown)
library(tidyverse)
library(patchwork)
```

## Overview

The basic idea here is that we would like to use the array system to process multiple jobs at one time. This will speed up analysis and will increase our potential to analyze a larger number of samples.

The `script` file contains the code needed to process a certain task but rather than calling one object, we can use the `$` as a place holder. In the `sbatch` file we have a specification for an array job. We also can specify how many samples to process on the array. The final piece of the puzzle is the `args` file which simply contains the objects of interest.

## OSCER Basics

1. Login to OSCER

```bach
ssh [YOUR_ACCOUNT_ID]@schooner.oscer.ou.edu
```

2. Activate environment

```bach
mamba activate /home/mbtoomey/.conda/envs/BIOL7263_Genomics
```

3. Submit a job to OSCER

```bach
sbatch /scratch/biol726310/BIOL7263_Genomics/array_project/scripts/[file_name.sbatch]
```

4. Check on a job

```bach
squeue -u biol726310
```

## Preliminary steps

- We can start by creating all of the directories needed throughout the downstream process.

- We can also create the scripts with generic names. For examples, we can simple call a sample `sample_1_1.fastq` and `sample_1_2.fastq`. This can be repeated for each sample being processed. This will keep things in a very simple way.

```bach
mmkdir alignment_data
mkdir assembly_data
mkdir blast_results
mkdir databases
mkdir databases/protein_db
mkdir databases/nucleotide_db
mkdir reference_seq
mkdir scripts
mkdir trimmed_reads
mkdir unmapped_reads
mkdir raw_data
```

- As a reminder, we may have to change environments throughout the process.

## Pulling data from the SRA

Below are the files submitted to OSCER:

1) **[sbatch file](array_scripts/fastq_dump_array.sbatch)** 

2) **[sh script](array_scripts/fastq_dump_array.sh)**

3) **[.args](array_scripts/fastq_dump.args)**

> DONE!

## Obtaining a reference genome (*Rosa*)

1) **[sbatch file](array_scripts/)** 

2) **[sh script](array_scripts/)**

3) **[.args](array_scripts/)**

> DONE!

We will likely need to use `gunzip` to decompress the files

## Quality control using fastqc (before trimming)

1) **[sh script](array_scripts/fastqc_array.sh)**

2) **[sbatch file](array_scripts/fastqc_array.sbatch)**

3) **[.args](array_scripts/fastqc_array.args)**

> DONE!

## Trimming

1) **[sh script](array_scripts/trimming_array.sh)** 

2) **[sbatch file](array_scripts/trimming_array.sbatch)**

3) **[.args](array_scripts/trimming_array.args)**

 
> DONE!

## Reference mapping of reads to host genome

Below I have included the script files for the `hisat2` mapping. This will require a change of the environment.

```bach
conda deactivate
conda activate hisat2_env
```

First we will need to generate an index. In the following step, we can use the index for the alignment.

1) **[sh script](array_scripts/mapping_array_index.sh)** 

2) **[sbatch file](array_scripts/mapping_array_index.sbatch)**

Next, we generate the alignment files (`.sam`) using the array system. The following code worked well. Reminder: We have to specify a generic output for each of the paired reads. Additionally, we have to keep each of the paired reads on the same line for this code to process. In the previous steps, we were able to put each of the arguments on seperate lines.

1) **[sh script](array_scripts/mapping_array.sh)** 

2) **[sbatch file](array_scripts/mapping_array.sbatch)**

3) **[.args](array_scripts/mapping_array.args)**

- create an index file
- we do not have to worry about `gzip` of the trimmed reads ()
- Aligning reads to the indexed reference genome

> DONE!

## Alignment manipulation and statistics

Convert the `.SAM` to `.BAM`
Sorting the `.BAM` file
Indexing the `.BAM` file

1) **[sh script](array_scripts/sam_bam_sort_array.sh)** 

2) **[sbatch file](array_scripts/sam_bam_sort_array.sbatch)**

3) **[.args](array_scripts/sam_bam_sort_array.args)**

> DONE!

## Gather unmapped reads using the flags

I was able to use both tools (bedtools and samtools) to generate fastq files. Below are the documents submitted to OSCER:

1) **[sh script](array_scripts/unmapped_reads_array.sh)** 

2) **[sbatch file](array_scripts/unmapped_reads_array.sbatch)**

3) **[.args](array_scripts/unmapped_reads_array.args)**

> DONE!

## Assembly of unmapped reads (rnaSPADES)

1) **[sh script](array_scripts/assembly_array.sh)**

2) **[sbatch file](array_scripts/assembly_array.sbatch)**

3) **[.args](array_scripts/assembly_array.args)**

> DONE!

## Downloading sequence data for Diamond searches and constructing the database

- I was able to incorporate the `mkdb` command as a separate line after pulling the protein sequence data. This puts everything into one script.

1) **[.sh script](array_scripts/protein_db_download.sh)**

2) **[.sbatch file](array_scripts/protein_db_download.sbatch)**

> DONE!

## Downloading sequence data for BLASTn searches and constructing the database

- I was able to incorporate the `makeblastdb` command as a separate line after pulling the nucleotide sequence data. This puts everything into one script.

1) **[.sh script](array_scripts/nucleotide_db_download.sh)**

2) **[.sbatch file](array_scripts/nucleotide_db_download.sh)**

- The command in this script appears to also construct the BLAST database, and no additional database making is required. We can simply begin our blast search.

- The blasting is still acting weird when trying to pull data from the nt_viruses folders. I may have to use the tar command to decompress all the files.

```
for file in nt_viruses.*.tar.gz; do
    tar -xzvf "$file"
done
```

## Blasting

1) **[.sh script](array_scripts/protein_nt_blasting.sh)** 

2) **[.sbatch file](array_scripts/protein_nt_blasting.sbatch)**

3) **[.args](array_scripts/protein_nt_blasting.args)**

- This command threw an error, but it seems that all of the blast stuff worked! 

> DONE!

## Analyze BLAST results for both protein and nucleotide blast

This portion of the analysis can be performed in R.


```{r}
df_1 <- read.csv("data_output/array_blast/nt_results_1.csv", header = FALSE)

colnames(df_1) <- c("NODE_Info","Query_ID","Seq_Title", "Percent_ID", "Align_Length", "Mismatch", "Gap_Open", "Query_Start", "Query_End", "Seq_Start","Send", "Evalue", "bitscore")

df_split <- df_1 %>% 
  mutate(NODE = str_extract(NODE_Info,"NODE_[0-9]+"),
         LENGTH = 
str_extract(Query_ID, "length_[0-9]+"),
         COV = 
str_extract(Query_ID, "cov_[0-9]+\\.[0-9]+"))
```




> Incomplete

> We need to check the number of NA's that are found in the searches and what these NA's are.
> We can also filter by e.value to remove low value (low probability) matches

I am thinking that I can add a new column (sample or something) so that I can generate an excel with all of the data in one file. This will be useful because I can filter by sample if I would like to. This will make analysis highly throughput.






















## Pfam?

I may try to use the pfam database (as described in the early portion of this course). This may be useful for the NA's sequences to see if there are any highly conserved protein regions that are of interest.


## Extracting relevant nodes/contigs

- EXAMPLE CODE
```{r}
library(Biostrings)
library(stringr)
library(tidyverse)
# Read the transcript FASTA file
fasta_file <- "data_output/transcripts.fasta"
transcripts <- readDNAStringSet(fasta_file)

# View the sequence IDs (headers)
transcript_ids <- names(transcripts)
head(transcript_ids)


# Extract unique IDs from the first column
nodes_of_interest <- unique(blast_results$V1)  # Adjust column number if needed
head(nodes_of_interest)

# Filter sequences based on nodes of interest
filtered_sequences <- transcripts[transcript_ids %in% nodes_of_interest]

# View filtered sequences
print(filtered_sequences)

# Write filtered sequences to a new FASTA file
output_file <- "filtered_transcripts.fasta"
writeXStringSet(filtered_sequences, filepath = output_file)
```

May have to separate the node portion


```{r}
# Extract the node name (assumes header format NODE_x_length_y_cov_z)
parsed_ids <- str_extract(transcript_ids, "NODE_\\d+")
head(parsed_ids)

# Filter using partial match
filtered_sequences <- transcripts[parsed_ids %in% nodes_of_interest]
```












