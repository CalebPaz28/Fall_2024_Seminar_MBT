---
title: "rrv_project_array"
author: "Paslay"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The basic idea here is that we would like to use the array system to process multiple jobs at one time. This will speed up analysis and will increase our potential to analyze a larger number of samples.

The `script` file contains the code needed to process a certain task but rather than calling one object, we can use the `$` as a place holder. In the `sbatch` file we have a specification for an array job. We also can specify how many samples to process on the array. The final piece of the puzzle is the `args` file which simply contains the objects of interest.

## OSCER Basics

1. Login to OSCER

```bach
ssh [YOUR_ACCOUNT_ID]@schooner.oscer.ou.edu
```

2. Activate environment

```bach
mamba activate /home/mbtoomey/.conda/envs/BIOL7263_Genomics
```

3. Submit a job to OSCER

```bach
sbatch /scratch/biol726310/BIOL7263_Genomics/array_project/scripts/[file_name.sbatch]
```

4. Check on a job

```bach
squeue -u biol726310
```

## Preliminary steps

- We can start by creating all of the directories needed throughout the downstream process.

- We can also create the scripts with generic names. For examples, we can simple call a sample `sample_1_1.fastq` and `sample_1_2.fastq`. This can be repeated for each sample being processed. This will keep things in a very simple way.

```bach
mmkdir alignment_data
mkdir assembly_data
mkdir databases
mkdir databases/protein_db
mkdir databases/nucleotide_db
mkdir reference_seq
mkdir scripts
mkdir trimmed_reads
mkdir unmapped_reads
mkdir raw_data
```

- As a reminder, we may have to change environments throughout the process.

## Pulling data from the SRA

Below are the files submitted to OSCER:

1) **[sbatch file](array_scripts/fastq_dump_array.sbatch)** 

2) **[sh script](array_scripts/fastq_dump_array.sh)**

3) **[.args](array_scripts/fastq_dump.args)**

> DONE!

## Obtaining a reference genome (*Rosa*)

1) **[sbatch file](array_scripts/)** 

2) **[sh script](array_scripts/)**

3) **[.args](array_scripts/)**

> DONE!

We will likely need to use `gunzip` to decompress the files

## Quality control using fastqc (before trimming)

1) **[sh script](array_scripts/fastqc_array.sh)**

2) **[sbatch file](array_scripts/fastqc_array.sbatch)**

3) **[.args](array_scripts/fastqc_array.args)**

> DONE!

## Trimming

1) **[sh script](array_scripts/trimming_array.sh)** 

2) **[sbatch file](array_scripts/trimming_array.sbatch)**

3) **[.args](array_scripts/trimming_array.args)**

 
> DONE!

## Reference mapping of reads to host genome

Below I have included the script files for the `hisat2` mapping. This will require a change of the environment.

```bach
conda deactivate
conda activate hisat2_env
```

First we will need to generate an index. In the following step, we can use the index for the alignment.

1) **[sh script](array_scripts/mapping_array_index.sh)** 

2) **[sbatch file](array_scripts/mapping_array_index.sbatch)**

Next, we generate the alignment files (`.sam`) using the array system. The following code worked well. Reminder: We have to specify a generic output for each of the paired reads. Additionally, we have to keep each of the paired reads on the same line for this code to process. In the previous steps, we were able to put each of the arguments on seperate lines.

1) **[sh script](array_scripts/mapping_array.sh)** 

2) **[sbatch file](array_scripts/mapping_array.sbatch)**

3) **[.args](array_scripts/mapping_array.args)**

- create an index file
- we do not have to worry about `gzip` of the trimmed reads ()
- Aligning reads to the indexed reference genome

> DONE!

## Alignment manipulation and statistics

Convert the `.SAM` to `.BAM`
Sorting the `.BAM` file
Indexing the `.BAM` file

1) **[sh script](array_scripts/sam_bam_sort_array.sh)** 

2) **[sbatch file](array_scripts/sam_bam_sort_array.sbatch)**

3) **[.args](array_scripts/sam_bam_sort_array.args)**

> DONE!

## Gather unmapped reads using the flags

I was able to use both tools (bedtools and samtools) to generate fastq files. Below are the documents submitted to OSCER:

1) **[sh script](array_scripts/unmapped_reads_array.sh)** 

2) **[sbatch file](array_scripts/unmapped_reads_array.sbatch)**

3) **[.args](array_scripts/unmapped_reads_array.args)**

> DONE!

## Assembly of unmapped reads (rnaSPADES)

1) **[sh script](array_scripts/assembly_array.sh)**

2) **[sbatch file](array_scripts/assembly_array.sbatch)**

3) **[.args](array_scripts/assembly_array.args)**

> DONE!

## Downloading sequence data for Diamond and BLASTn searches

1) **[.sh script](array_scripts/)**

2) **[.sbatch file](array_scripts/)**

3) **[.args](array_scripts/)**

> The download process is working with our current script using the web link. I changed the output location to the protein_db folder so now that would work well. If this works, I will then attempt to download the nucleotide as a seperate link of code but within the same script.

## Constructing the protein and nucleotide databases

I may have to use the `mkdb` commands separately.

> Incomplete

## Blasting

1) **[.sh script](array_scripts/)** 

2) **[.sbatch file](array_scripts/)**

3) **[.args](array_scripts/)**

> I will set the search parameter to 3 search per node (to reduce computational requirements)
> Incomplete

## Analyze BLAST results for both protein and nucleotide blast

This portion of the analysis can be performed in R.

> Incomplete

> We need to check the number of NA's that are found in the searches and what these NA's are.
> We can also filter by e.value to remove low value (low probability) matches

I am thinking that I can add a new column (sample or something) so that I can generate an excel with all of the data in one file. This will be useful because I can filter by sample if I would like to. This will make analysis highly throughput.


## Pfam?

I may try to use the pfam database (as described in the early portion of this course). This may be useful for the NA's sequences to see if there are any highly conserved protein regions that are of interest.


## Extracting relevant nodes/contigs















